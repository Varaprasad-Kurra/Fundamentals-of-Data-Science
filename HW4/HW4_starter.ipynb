{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "In this homework assignment, your friends in forecast department are in trouble and you will help them implement a number of performance metrics used in evaluating their forecasts. \n",
    "\n",
    "There are many types of forecasts, each of which calls for slightly different methods of verification. Your friends are primarily interested in dichotomous forecasts. Quick research leads you to the fact that it is essentially binary prediction. \n",
    "\n",
    "Their terminology is slightly different. Following are the differences:\n",
    "- Instead of True Positives (TP) --> they use _Hits_ (a), \n",
    "- Instead of False Positives (FP) --> they use _False Alarms_ (b)\n",
    "- Instead of False Negatives (FN) --> they use _Misses_ (c), \n",
    "- Instead of True Negatives (TN) --> they use _Correct Rejects_ (d).\n",
    "\n",
    "For every single model run, you are given:\n",
    "1. a set of observations (Event [1] or No Event [0]) \n",
    "2. a set of prediction scores (a float between 0 and 1) and an event threshold, where the predicted outcome will be \n",
    "\n",
    "<center>$\\hat{y}= \n",
    "\\begin{cases}\n",
    "    1 \\text{ (Event occurred)},& \\text{if } score \\geq threshold\\\\\n",
    "    0 \\text{ (No event),}      & \\text{if } score < threshold\n",
    "\\end{cases} $ <center>\n",
    "\n",
    "Note that observations are ground truth and prediction scores and threshold will be used for determining the predicted model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Here are some test data you can use for this homework\n",
    "Y_test = [0, 0, 1, 1, 0, 1, 0, 0, 0, 1] # observations\n",
    "P_scores = [0.1, 0.32, 0.48, 0.9, 0.6, 0.55, 0.42, 0.37, 0.61, 0.66] # prediction scores\n",
    "threshold = 0.5 # prediction threshold\n",
    "\n",
    "P_scores1 = P_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (20 points)\n",
    "Given prediction scores, threshold, and observations create a function to determine the elements of a confusion matrix. For ease of use, you will output a `dict` (dictionary) object instead of a 2-dimensional numpy array. Note that _positive_ corresponds to the event occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty dictionary with intial Values \n",
      "{'TP': 0, 'FN': 0, 'FP': 0, 'TN': 0}\n",
      "\n",
      "\n",
      "Updated P_scores based on threshold Values\n",
      "[0, 0, 0, 1, 1, 1, 0, 0, 1, 1]\n",
      "\n",
      "Confusion Matrix is:\n",
      "[[4 1]\n",
      " [2 3]]\n",
      "TP =  4\n",
      "FP =  1\n",
      "FN =  2\n",
      "TN =  3\n",
      "\n",
      "Updated Dictionary with Confusion Matrix values\n",
      "{'TP': 4, 'FN': 1, 'FP': 2, 'TN': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def binary_conf_matrix(observation, p_scores, threshold):\n",
    "    '''Finds the entries (TP, FP, FN, TN) in a binary confusion matrix for forecast problems\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    observation: list\n",
    "        list of observations (1 is there is event, 0 is there is no event)\n",
    "    p_scores: list\n",
    "        list of prediction scores (scores vary between 0 and 1)\n",
    "    threshold: float\n",
    "        threshold that will be used for binary outcome from prediction scores\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        a dictionary that shows the counts for TP, TN, FP, and FNs.\n",
    "    \n",
    "    '''\n",
    "    print('Empty dictionary with intial Values ')\n",
    "    bcmd = {'TP':0, 'FN':0, 'FP':0, 'TN':0}\n",
    "    print(bcmd)\n",
    "    print('')\n",
    "    \n",
    "    for index, item in enumerate(P_scores):\n",
    "        if item <0.5:\n",
    "            P_scores[index] = 0\n",
    "        elif item >= 0.5:\n",
    "            P_scores[index] = 1\n",
    "    print('')\n",
    "    print('Updated P_scores based on threshold Values')\n",
    "    print(P_scores)\n",
    "    \n",
    "    print('')\n",
    "    print('Confusion Matrix is:')\n",
    "    cm = confusion_matrix(P_scores,Y_test)\n",
    "    print(cm)\n",
    "    print('TP = ',cm[0][0])\n",
    "    print('FP = ',cm[0][1])\n",
    "    print('FN = ',cm[1][0])\n",
    "    print('TN = ',cm[1][1])\n",
    "    \n",
    "    print('')\n",
    "    bcmd = {'TP':cm[0][0],'FN':cm[0][1],'FP':cm[1][0],'TN':cm[1][1]}\n",
    "    print('Updated Dictionary with Confusion Matrix values')\n",
    "    print(bcmd)\n",
    "    \n",
    "    return bcmd\n",
    "\n",
    "#test your code\n",
    "bcmd = binary_conf_matrix( Y_test, P_scores, threshold )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next six questions will use the same input parameters that are used in `binary_conf_matrix` function. Please refer to the Q1 starter code documentation for the input parameters of the Questions 2-7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (20 points)\n",
    "\n",
    "Create functions for calculating accuracy, precision, recall, and F1-score. You can use the definitions from slides (Chapter 10). (You are supposed to calculate the precision and recall (and thus F1-score) for 'Event' [1] class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of count in Confusion_Matrix is:  10\n",
      "\n",
      "Accucary is  0.7\n",
      "Precision is  0.6666666666666666\n",
      "Recall is  0.8\n",
      "F1_Score is  0.73\n"
     ]
    }
   ],
   "source": [
    "total = sum(bcmd.values())\n",
    "print('Sum of count in Confusion_Matrix is: ',total)\n",
    "print('')\n",
    "\n",
    "\n",
    "def accuracy(observation, p_scores, threshold):\n",
    "    accuracy = (bcmd['TP'] + bcmd['TN'])/total\n",
    "    return accuracy \n",
    "\n",
    "def precision(observation, p_scores, threshold):\n",
    "    precision = bcmd['TP'] / (bcmd['TP']+bcmd['FP'])\n",
    "    return precision\n",
    "\n",
    "def recall(observation, p_scores, threshold):\n",
    "    recall = bcmd['TP'] / (bcmd['TP']+bcmd['FN'])\n",
    "    return recall\n",
    "\n",
    "def f1score(observation, p_scores, threshold):\n",
    "    f1score = float(2*precision*recall)/(precision+recall)\n",
    "    return f1score\n",
    "\n",
    "\n",
    "# test your code using the following\n",
    "accuracy = accuracy( Y_test, P_scores, threshold )\n",
    "print('Accucary is ',accuracy)\n",
    "precision = precision( Y_test, P_scores, threshold )\n",
    "print('Precision is ',precision)\n",
    "recall = recall( Y_test, P_scores, threshold )\n",
    "print('Recall is ',round(recall,2))\n",
    "f1score = f1score( Y_test, P_scores, threshold )\n",
    "print('F1_Score is ',round(f1score,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (10 points)\n",
    "Calculate the bias score (BIAS). Bias score measures the ratio of the frequency of predicted event occurrences to the frequency of observed events. It can be calculated using the following formula:\n",
    "\n",
    "$ BIAS = \\frac{\\text{hits} + \\text{false alarms} }{ \\text{hits} + \\text{misses} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 4, 'FN': 1, 'FP': 2, 'TN': 3}\n",
      "{'Hits': 4, 'Misses': 1, 'False_Alarm': 2, 'Correct_Rejects': 3}\n",
      "Bias is  1.2\n"
     ]
    }
   ],
   "source": [
    "# bias score\n",
    "\n",
    "bcmd1 = bcmd.copy()\n",
    "print(bcmd1)\n",
    "\n",
    "bcmd1['Hits'] = bcmd1.pop('TP')\n",
    "bcmd1['Misses'] = bcmd1.pop('FN')\n",
    "bcmd1['False_Alarm'] = bcmd1.pop('FP')\n",
    "bcmd1['Correct_Rejects'] = bcmd1.pop('TN')\n",
    "print(bcmd1)\n",
    "\n",
    "def bias_score(observation, p_scores, threshold):\n",
    "    bias = (bcmd1['Hits'] + bcmd1['False_Alarm']) / (bcmd1['Hits']+bcmd1['Misses'])\n",
    "    return bias\n",
    "\n",
    "\n",
    "# test your code using the following\n",
    "bias = bias_score(Y_test, P_scores, threshold)\n",
    "print('Bias is ',round(bias,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (10 points)\n",
    "\n",
    "Calculate the false alarm ratio (FAR). FAR is an indicator of what fraction of the predicted events actually did not occur (i.e., they were false alarms). You can calculate the FAR as follows: \n",
    "\n",
    "$ FAR = \\frac{ \\text{false alarms} }{ \\text{hits} + \\text{false alarms} }  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false alarm ratio is  0.33\n"
     ]
    }
   ],
   "source": [
    "def far(observation, p_scores, threshold):\n",
    "    #your code goes here\n",
    "    far = bcmd1['False_Alarm']/(bcmd1['Hits'] + bcmd1['False_Alarm'])\n",
    "    return far\n",
    "\n",
    "# test your code using the following\n",
    "far = far(Y_test, P_scores, threshold)\n",
    "print('false alarm ratio is ', round(far,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (10 points)\n",
    "Calculate the threat score. Threat score (which is also referred to as critical success index -- CSI) indicates how well did the predicted event outcomes correspond to the observed events. It measures the fraction of actual __and/or__ predicted events that were correctly predicted. It can be thought of as the accuracy when the correct negatives (TN) have been removed from consideration. It can be calculated as follows:\n",
    "\n",
    "$CSI = \\frac{ \\text{hits} }{ \\text{hits} + \\text{misses} + \\text{false alarms} } $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threat score is  0.57\n"
     ]
    }
   ],
   "source": [
    "def csi(observation, p_scores, threshold):\n",
    "    csi = bcmd1['Hits']/(bcmd1['Hits']+bcmd1['Misses']+bcmd1['False_Alarm'])\n",
    "    return csi \n",
    "\n",
    "\n",
    "# test your code using the following\n",
    "threat_score = csi(Y_test, P_scores, threshold)\n",
    "print('threat score is ',round(threat_score,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (15 points)\n",
    "Skill scores are often used to understand the improvement of model performance over a given baseline (often a hypothetical, predetermined random forecast result). The first skill score you will implement is _true skill statistic (TSS)_ (which is also known as Hanssen-Kuipers Skill Score. It can be used to understand how well the prediction model separate the events (detected) from the no events. TSS can be calculated as follows:\n",
    "\n",
    "$ TSS = \\frac{\\text{hits} }{\\text{hits} + \\text{misses}} - \\frac{\\text{false alarms} }{\\text{false alarms} + \\text{correct negatives}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true skill statistic is 0.4\n"
     ]
    }
   ],
   "source": [
    "def tss(observation, p_scores, threshold):\n",
    "    tss = (bcmd1['Hits']/(bcmd1['Hits']+bcmd1['Misses'])) - (bcmd1['False_Alarm']/(bcmd1['False_Alarm']+bcmd1['Correct_Rejects'])) \n",
    "    return tss\n",
    "\n",
    "# test your code using the following\n",
    "true_skill_statistic = tss(Y_test, P_scores, threshold)\n",
    "print('true skill statistic is',round(true_skill_statistic,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 (15 points)\n",
    "The next skill score you will calculate is Gilbert Skill Score (also known as Equitable Threat Score). This is an interesting indicator of performance because it measures the fraction of observed and/or predicted events that were correctly predicted, adjusted for hits (correctly predicted events) associated with random chance. For example, it is easier to correctly predict rain occurrence in a wet climate than in a dry climate. In other words, GSS can answer how well the predicted event occurrences correspond to the observed events (accounting for correct predictions appearing due to chance). The Gilbert Skill Score can be calculated as follows:\n",
    "\n",
    "$GSS = \\frac{ \\text{hits} - \\text{hits}_{random}}{ \\text{hits} + \\text{misses} + \\text{false alarms} - \\text{hits}_{random} } $\n",
    "\n",
    "where the random hits can be calculated as:\n",
    "\n",
    "$ \\text{hits}_{random} = \\frac{ (\\text{hits}+\\text{misses} )* (\\text{hits}+\\text{false alarms} )}{total} $.\n",
    "\n",
    "_total_ is sum of all the elements in confusion matrix. \n",
    "\n",
    "Hint: Notice the similarity between GSS and threat score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gilbert Skill Score is  0.25\n"
     ]
    }
   ],
   "source": [
    "def gss(observation, p_scores, threshold):\n",
    "    total = sum(bcmd1.values()) \n",
    "    hits_random = (( bcmd1['Hits'] +bcmd1['Misses'] )*( bcmd1['Hits']+bcmd1['False_Alarm']))/total\n",
    "    gss = (bcmd1['Hits'] - hits_random)/( bcmd1['Hits'] + bcmd1['Misses'] + bcmd1['False_Alarm'] - hits_random)\n",
    "    return gss\n",
    "\n",
    "\n",
    "# test your code using the following\n",
    "Gilbert_Skill_Score = gss(Y_test, P_scores, threshold)\n",
    "print('Gilbert Skill Score is ', Gilbert_Skill_Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question (10 points)\n",
    "\n",
    "Your last task is to determine an optimal threshold based on prediction scores, observations, and a given performance measure. Create a function called `pick_threshold` which will pick the best prediction score threshold (that will return the highest performance measure value based on the given performance metric). Hint: Python allows you to pass a (performance measure) function (such as `tss`, `csi`, or `f1score`) to `pick_threshold`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intial Score is:  0.5714285714285714\n",
      "New Limit is   0.55\n",
      "new_score is  0.5714285714285714\n",
      "New Limit is   0.6000000000000001\n",
      "new_score is  0.5714285714285714\n",
      "New Limit is   0.6500000000000001\n",
      "new_score is  0.5714285714285714\n",
      "New Limit is   0.7000000000000002\n",
      "new_score is  0.5714285714285714\n",
      "New Limit is   0.7500000000000002\n",
      "new_score is  0.5714285714285714\n",
      "New Limit is   0.8000000000000003\n",
      "new_score is  0.5714285714285714\n",
      "New Limit is   0.8500000000000003\n",
      "new_score is  0.5714285714285714\n",
      "New Limit is   0.9000000000000004\n",
      "new_score is  0.5714285714285714\n",
      "New Limit is   0.9500000000000004\n",
      "new_score is  0.5714285714285714\n",
      "New Limit is   1.0000000000000004\n",
      "new_score is  0.5714285714285714\n",
      "[0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714]\n",
      "[0.55, 0.6000000000000001, 0.6500000000000001, 0.7000000000000002, 0.7500000000000002, 0.8000000000000003, 0.8500000000000003, 0.9000000000000004, 0.9500000000000004, 1.0000000000000004]\n",
      "-----------------------------\n",
      "Intial Score is:  0.4\n",
      "New Limit is   0.55\n",
      "new_score is  0.4\n",
      "New Limit is   0.6000000000000001\n",
      "new_score is  0.4\n",
      "New Limit is   0.6500000000000001\n",
      "new_score is  0.4\n",
      "New Limit is   0.7000000000000002\n",
      "new_score is  0.4\n",
      "New Limit is   0.7500000000000002\n",
      "new_score is  0.4\n",
      "New Limit is   0.8000000000000003\n",
      "new_score is  0.4\n",
      "New Limit is   0.8500000000000003\n",
      "new_score is  0.4\n",
      "New Limit is   0.9000000000000004\n",
      "new_score is  0.4\n",
      "New Limit is   0.9500000000000004\n",
      "new_score is  0.4\n",
      "New Limit is   1.0000000000000004\n",
      "new_score is  0.4\n",
      "[0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n",
      "[0.55, 0.6000000000000001, 0.6500000000000001, 0.7000000000000002, 0.7500000000000002, 0.8000000000000003, 0.8500000000000003, 0.9000000000000004, 0.9500000000000004, 1.0000000000000004]\n"
     ]
    }
   ],
   "source": [
    "def pick_threshold(observation, P_scores, mfunc):\n",
    "    '''Finds the prediction score threshold that returns the highest performance measure value \n",
    "    based on a given measure.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    observation: list\n",
    "        list of observations (1 is there is event, 0 is there is no event)\n",
    "    p_scores: list\n",
    "        list of prediction scores (scores vary between 0 and 1)\n",
    "    mfunc: function\n",
    "        one of the performance measurement functions that will take (observation, p_scores, threshold) as\n",
    "        the list of arguments. (Hint: one can call mfunc(observation, p_scores, threshold) within the \n",
    "        function scope.)\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        threshold value that gives the best performance based on mfunc\n",
    "    \n",
    "    '''\n",
    "    #your code goes here\n",
    "    score_list =[]\n",
    "    limit1 = []\n",
    "    intial_score = mfunc(observation, P_scores, threshold)\n",
    "    print('Intial Score is: ',intial_score)\n",
    "    limit = threshold\n",
    "    while(limit<=1):\n",
    "        limit = limit+0.05\n",
    "        print('New Limit is  ',limit)\n",
    "        limit1.append(limit)\n",
    "        score1 = mfunc(observation, P_scores, limit)\n",
    "        print('new_score is ',score1)\n",
    "        score_list.append(score1)\n",
    "    print(score_list)\n",
    "    print(limit1)\n",
    "#test your code using the following\n",
    "pick_threshold(Y_test, P_scores, csi)\n",
    "print('-----------------------------')\n",
    "pick_threshold(Y_test, P_scores, tss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
